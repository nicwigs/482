Big data is a field that treats of ways to analyze, systematically 
extract information from, or otherwise deal with data sets that are too 
large or complex to be dealt with by traditional data-processing 
application software. Data with many cases (rows) offer greater 
statistical power, while data with higher complexity (more attributes or 
columns) may lead to a higher false discovery rate.[2] Big data 
challenges include capturing data, data storage, data analysis, search, 
sharing, transfer, visualization, querying, updating, information 
privacy and data source. Big data was originally associated with three 
key concepts: volume, variety, and velocity.[3] Other concepts later 
attributed with big data are veracity (i.e., how much noise is in the 
data) [4] and value.[5]

Current usage of the term big data tends to refer to the use of 
predictive analytics, user behavior analytics, or certain other advanced 
data analytics methods that extract value from data, and seldom to a 
particular size of data set. "There is little doubt that the quantities 
of data now available are indeed large, but that's not the most relevant 
characteristic of this new data ecosystem."[6] Analysis of data sets can 
find new correlations to "spot business trends, prevent diseases, combat 
crime and so on."[7] Scientists, business executives, practitioners of 
medicine, advertising and governments alike regularly meet difficulties 
with large data-sets in areas including Internet search, fintech, urban 
informatics, and business informatics. Scientists encounter limitations 
in e-Science work, including meteorology, genomics,[8] connectomics, 
complex physics simulations, biology and environmental research.[9]

Data sets grow rapidly- in part because they are increasingly gathered 
by cheap and numerous information- sensing Internet of things devices 
such as mobile devices, aerial (remote sensing), software logs, cameras, 
microphones, radio-frequency identification (RFID) readers and wireless 
sensor networks.[10][11] The world's technological per-capita capacity 
to store information has roughly doubled every 40 months since the 
1980s;[12] as of 2012, every day 2.5 exabytes (2.5Ã—1018) of data are 
generated.[13] Based on an IDC report prediction, the global data volume 
will grow exponentially from 4.4 zettabytes to 44 zettabytes between 
2013 and 2020.[14] By 2025, IDC predicts there will be 163 zettabytes of 
data.[15] One question for large enterprises is determining who should 
own big-data initiatives that affect the entire organization.[16]

Relational database management systems, desktop statistics[clarification 
needed] and software packages used to visualize data often have 
difficulty handling big data. The work may require "massively parallel 
software running on tens, hundreds, or even thousands of servers".[17] 
What qualifies as being "big data" varies depending on the capabilities 
of the users and their tools, and expanding capabilities make big data a 
moving target. "For some organizations, facing hundreds of gigabytes of 
data for the first time may trigger a need to reconsider data management 
options. For others, it may take tens or hundreds of terabytes before 
data size becomes a significant consideration."[18]
